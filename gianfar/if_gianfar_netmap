/*
 * $ID: if_gfar_netmap.h 02-09-2015 Freescale Semiconductor Inc $
 * netmap support for: ETSEC(gianfar) (linux version)
 * For details on netmap support please see ixgbe_netmap.h
 */


#include <bsd_glue.h>
#include <net/netmap.h>
#include <netmap/netmap_kern.h>

#define SOFTC_T	gfar_private

/*
 * Register/unregister. We are already under netmap lock.
 */
static int
gfar_netmap_reg(struct netmap_adapter *na, int onoff)
{
  struct ifnet *ifp = na->ifp;
  struct SOFTC_T *priv = netdev_priv(ifp);

 
  while(test_and_set_bit_lock(GFAR_RESETTING, &priv->state))
    cpu_relax();
  
  if (ifp->flags & IFF_UP)
    stop_gfar(ifp);

  /* enable or disable flags and callbacks in na and ifp */
  if (onoff) {
    nm_set_native_flags(na);
  } else {
    nm_clear_native_flags(na);
  }

  if (ifp->flags & IFF_UP)
  {
	D("Starting gfar");
 	startup_gfar(ifp);
	D("After startup_gfar");
  }
  clear_bit_unlock(GFAR_RESETTING, &priv->state);

  return 0;

}

static int
gfar_netmap_txsync(struct netmap_kring *kring, int flags)
{
  	struct netmap_adapter *na = kring->na;
	struct ifnet *ifp = na->ifp;
	struct netmap_ring *ring = kring->ring;
	u_int nm_i;	/* index into the netmap ring */
	u_int nic_i, nic_tail;	/* index into the NIC ring */
	u_int n;
	u_int const lim = kring->nkr_num_slots - 1;
	u_int const head = kring->rhead;
	uint32_t lstatus;
	struct SOFTC_T *priv = netdev_priv(ifp);
	struct txbd8 *bd_start = NULL;
	struct gfar_priv_tx_q *txq  = priv->tx_queue[kring->ring_id];
	struct gfar __iomem *regs = priv->gfargrp[kring->ring_id].regs;
	/*
	 * First part: process new packets to send.
	 */


	if (!netif_carrier_ok(ifp)) {
		D("netif carrier not ok");
		goto out;
	}

	nm_i = kring->nr_hwcur;
	if (nm_i != head) {	/* we have new packets to send */
	  nic_i = netmap_idx_k2n(kring, nm_i);
 
		for (n = 0; nm_i != head; n++) {
 			struct netmap_slot *slot = &ring->slot[nm_i];
			u_int len = slot->len;
			uint64_t paddr;
			void *addr = PNMB(na, slot, &paddr);

		 	/* device-specific */
			struct txbd8 *bd = txq->tx_bd_base + nic_i;
			uint32_t flags = (slot->flags & NS_REPORT) ? BD_LFLAG(TXBD_INTERRUPT) : 0;
			
 			NM_CHECK_ADDR_LEN(na, addr, len);

			if (slot->flags & NS_BUF_CHANGED) {
				/* buffer has changed, reload map */
				// netmap_reload_map(pdev, DMA_TO_DEVICE, old_paddr, addr);
			  bd->bufPtr =(uint32_t)dma_map_single(priv->dev, addr, 2048/*hardcoded*/, DMA_FROM_DEVICE);;
			}
			slot->flags &= ~(NS_REPORT | NS_BUF_CHANGED);

			/* Update the lstatus variable in the NIC ring. */
			if(n==0)
			  {
			    bd_start = bd;
			   
			    lstatus = bd->lstatus | flags | len | BD_LFLAG(TXBD_CRC | TXBD_READY);
			    
			    if(!(slot->flags & NS_MOREFRAG))
			 	lstatus |= BD_LFLAG(TXBD_LAST);	 
			 }
			else
			{
			  bd->lstatus = cpu_to_be32(bd->lstatus | flags | len | BD_LFLAG(TXBD_CRC | TXBD_READY));
			  
			  if(!(slot->flags & NS_MOREFRAG))
                         	bd->lstatus |= BD_LFLAG(TXBD_LAST);
			}
			nm_i = nm_next(nm_i, lim);
			nic_i = nm_next(nic_i, lim);
		}

		kring->nr_hwcur = head;
		eieio();	/* synchronize writes to the NIC ring */
		bd_start->lstatus = cpu_to_be32(lstatus);
		eieio();
		/* restart tx */
  		gfar_write(&regs->tstat, TSTAT_CLEAR_THALT >> txq->qindex);
	
	}
	/*
         * Second part: reclaim buffers for completed transmissions
         */
	nic_i = netmap_idx_k2n(kring, head);
	nm_i = kring->nr_hwtail;
	nic_tail = nm_next(netmap_idx_k2n(kring, nm_i), lim);
	for(n=0; nic_tail != nic_i; n++)
	  {
	    struct txbd8 *bd = (struct txbd8*)txq->tx_bd_base + nic_tail;
	    if(bd->lstatus & BD_LFLAG(TXBD_READY))
	       break;

	     nic_tail = nm_next(nic_tail, lim);
	  }

	if(n > 0)
	  kring->nr_hwtail = nm_prev(netmap_idx_n2k(kring, nic_tail), lim);
 out:
	nm_txsync_finalize(kring);
	return 0;
}

static int
gfar_netmap_rxsync(struct netmap_kring *kring, int flags)
{
  struct netmap_adapter *na = kring->na;
  struct ifnet *ifp = na->ifp;
  struct netmap_ring *ring = kring->ring;
  u_int ring_nr = kring->ring_id;
  u_int nm_i;     /* index into the netmap ring */
  u_int nic_i;    /* index into the NIC ring */
  u_int n;
  u_int const lim = kring->nkr_num_slots - 1;
  u_int const head = nm_rxsync_prologue(kring);
  int force_update = (flags & NAF_FORCE_READ) || kring->nr_kflags & NKR_PENDINTR;

  /* device-specific */
  struct SOFTC_T *priv = netdev_priv(ifp);
  struct gfar_priv_rx_q  *rxq = priv->rx_queue[ring_nr];
  struct rxbd8 *bd = NULL;
  
  if (!netif_carrier_ok(ifp)) {
    goto out;
  }

  if (head > lim)
    return netmap_ring_reinit(kring);
  /*
   * First part: import newly received packets.
   */
 //D("RX Ring:%d", kring->ring_id);
 
   if (netmap_no_pendintr || force_update) {
    uint16_t slot_flags = kring->nkr_slot_flags;
 
    nm_i = kring->nr_hwtail;
    nic_i = netmap_idx_k2n(kring, nm_i);
    
    for(n=0; ;n++){
      bd = rxq->rx_bd_base + nic_i;
      rmb();
      if ((bd->status & RXBD_EMPTY) || (bd->status & RXBD_RO1))
	break;                                            
      ring->slot[nm_i].len = bd->length - ETH_FCS_LEN;
      ring->slot[nm_i].flags = slot_flags;
      //D("Acquired buffer:%d", nic_i);
      nm_i = nm_next(nm_i, lim);
      nic_i = nm_next(nic_i, lim);
 }
 //D("Accquired %d buffers", n);   
    if (n) { /* update the state variables */
      kring->nr_hwtail = nm_i;
      //bd->lstatus = bd->lstatus & ~(BD_LFLAG(RXBD_RO1));
      //gfar_wmb();
    }
    kring->nr_kflags &= ~NKR_PENDINTR;
  }
  
  /*
   * Second part: skip past packets that userspace has released.
   */
  nm_i = kring->nr_hwcur;
  n=0;
  if(nm_i!=head)
    {
      nic_i = netmap_idx_k2n(kring, nm_i);
      for(n=0;nm_i!=head;n++)
	{
	  struct netmap_slot *slot = &ring->slot[nm_i];
	  uint64_t paddr;
	  void *addr = PNMB(na, slot, &paddr);
	  bd = rxq->rx_bd_base + nic_i;

	  if (addr == NETMAP_BUF_BASE(na)) /* bad buf */
	    goto ring_reset;
	  if (slot->flags & NS_BUF_CHANGED) {
	    // netmap_reload_map(...)
	    bd->bufPtr = (uint32_t)dma_map_single(priv->dev, addr, 2048/*hardcoded*/, DMA_FROM_DEVICE);;
	    slot->flags &= ~NS_BUF_CHANGED;
	  }
	  bd->lstatus |= BD_LFLAG(RXBD_EMPTY);
	  //D("released buffer:%d", nic_i);
	  nm_i = nm_next(nm_i, lim);
	  nic_i = nm_next(nic_i, lim);

	}
      nic_i = nm_prev(nic_i, lim);
      bd = rxq->rx_bd_base + nic_i;
      eieio();
      bd->lstatus  = (bd->lstatus & ~(BD_LFLAG(RXBD_EMPTY))) | BD_LFLAG(RXBD_RO1);
      eieio();
      //nic_i = netmap_idx_k2n(kring, kring->nr_hwtail);
      bd = rxq->cur_rx;
      bd->lstatus  = (bd->lstatus & ~(BD_LFLAG(RXBD_RO1))) | BD_LFLAG(RXBD_EMPTY);
      eieio();
      rxq->cur_rx = rxq->rx_bd_base + nic_i;
      kring->nr_hwcur = head;
    }
    //D("Released %d buffers", n);
out:
  nm_rxsync_finalize(kring);
  return 0;

ring_reset:
  return netmap_ring_reinit(kring);
}


static int
gfar_netmap_init_bds(struct SOFTC_T *priv)
{
  struct ifnet *ifp = priv->ndev;
  struct netmap_adapter *na = NA(ifp);
  struct netmap_slot *slot;
  struct txbd8 *txbdp;
  struct rxbd8 *rxbdp;
  int i, j, lim;
  uint64_t bufaddr;  /*Not sure if this should be uint64_t or dma_addr_t*/
  void *vbufaddr;	
  /*Check if the netmap mode is turned ON
   *If not return zero which will allow 
   *the driver to carry on with the normal 
   *initialization*/
  if(!nm_native_on(na))
    return 0;

   for (i = 0; i < na->num_tx_rings; i++) {
    /*for TX buffer pool descriptors*/
    slot = netmap_reset(na, NR_TX, i, 0);
    if(!slot)
      return 0;
    txbdp = priv->tx_queue[i]->tx_bd_base;

    for(j=0;j<na->num_tx_desc;j++,txbdp++)
      {
	int l = netmap_idx_n2k(&na->tx_rings[i], j);
 	vbufaddr = NMB(na, slot+l);
  	txbdp->lstatus = 0;
	txbdp->bufPtr = (uint32_t)dma_map_single(priv->dev, vbufaddr, 2048/*hardcoded*/, DMA_FROM_DEVICE);
      }
    txbdp--;
    txbdp->lstatus = BD_LFLAG(TXBD_WRAP);	
  }
    /*for RX buffer pool descriptors*/
  for(i=0;i<na->num_rx_rings;i++) {
    slot = netmap_reset(na, NR_RX, i, 0);
    if(!slot)
      return 0;
    rxbdp = priv->rx_queue[i]->rx_bd_base;
    lim = na->num_rx_desc -1 - nm_kr_rxspace(&na->rx_rings[i]);
    for(j=0;j<na->num_rx_desc;j++)
      {
	int l = netmap_idx_n2k(&na->rx_rings[i], j);
	vbufaddr = NMB(na, slot+l);
	if(j<lim)
	  gfar_init_rxbdp(priv->rx_queue[i], rxbdp, (uint32_t)dma_map_single(priv->dev, vbufaddr, 2048/*hardcoded*/, DMA_FROM_DEVICE));
	else
	  {
	    uint32_t lstatus;
	    priv->rx_queue[i]->cur_rx = rxbdp;
	    rxbdp->bufPtr = (uint32_t)dma_map_single(priv->dev, vbufaddr, 2048/*hardcoded*/, DMA_FROM_DEVICE); 
 	    lstatus = BD_LFLAG(RXBD_RO1 | RXBD_INTERRUPT);/*Need RXBD_RO1 to break out of loops*/
	    if (j==na->num_rx_desc-1)
	      lstatus |= BD_LFLAG(RXBD_WRAP);
	    rxbdp->lstatus = lstatus;
	  }  
	rxbdp++;
      }
  }
  return 1;    
}

static void
gfar_netmap_attach(struct SOFTC_T *priv)
{
	struct netmap_adapter na;

	bzero(&na, sizeof(na));

	na.ifp = priv->ndev;
	na.pdev = priv->dev;
	na.num_tx_desc = priv->tx_queue[0]->tx_ring_size;
	na.num_rx_desc = priv->rx_queue[0]->rx_ring_size;
	na.nm_txsync = gfar_netmap_txsync;
	na.nm_rxsync = gfar_netmap_rxsync;
	na.nm_register = gfar_netmap_reg;/*TODO: Implement this function*/
	na.num_tx_rings = priv->num_tx_queues;
	na.num_rx_rings = priv->num_rx_queues;
	netmap_attach(&na);
}

